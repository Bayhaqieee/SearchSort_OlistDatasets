{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeWz/VfEjAuLX8E5o/cAPU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bayhaqieee/SearchSort_OlistDatasets/blob/main/SearchSort_Olist_Team_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR4ZoyyMoe3u",
        "outputId": "99de2362-1b6d-4c22-b9f8-61163e766e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Search - Counting Sort"
      ],
      "metadata": {
        "id": "DwLRkpbI489a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4mn20fLdNkU",
        "outputId": "34025403-446c-444a-ffbd-a9b88334fcae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+---------+-----------------------------+--------------+--------------+--------------------+------------+-----------------------+----------------+----------------+----------------------+--------------+-------------------+--------------------+--------------------+--------------------------+------------------+\n",
            "|    | Dataset             |    Rows | ID Column                   | ID Used As   |   Sort by ID |   Binary Search ID |   Total ID | Name Column           | Name Used As   |   Sort by Name |   Binary Search Name |   Total Name | Quantity Column   | Quantity Used As   |   Sort by Quantity |   Binary Search Quantity |   Total Quantity |\n",
            "+====+=====================+=========+=============================+==============+==============+====================+============+=======================+================+================+======================+==============+===================+====================+====================+==========================+==================+\n",
            "|  0 | Geolocation Dataset | 1000163 | geolocation_zip_code_prefix | ID           |     0.214325 |            3.1e-05 |   0.214356 | geolocation_city      | Name           |        1.081   |              1.8e-05 |     1.08102  | geolocation_lat   | Quantity           |           0.127878 |                  8.6e-05 |         0.127964 |\n",
            "+----+---------------------+---------+-----------------------------+--------------+--------------+--------------------+------------+-----------------------+----------------+----------------+----------------------+--------------+-------------------+--------------------+--------------------+--------------------------+------------------+\n",
            "|  1 | Products Dataset    |   32951 | product_id                  | ID           |     0.028585 |            1.7e-05 |   0.028602 | product_category_name | Name           |        0.02073 |              5e-06   |     0.020735 | product_length_cm | Quantity           |           0.003031 |                  1.6e-05 |         0.003047 |\n",
            "+----+---------------------+---------+-----------------------------+--------------+--------------+--------------------+------------+-----------------------+----------------+----------------+----------------------+--------------+-------------------+--------------------+--------------------+--------------------------+------------------+\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load dataset\n",
        "GEO_URL = '/content/drive/MyDrive/Kuliah/Analgo/Dataset/olist_geolocation_dataset.csv'\n",
        "PROD_URL = '/content/drive/MyDrive/Kuliah/Analgo/Dataset/olist_products_dataset.csv'\n",
        "\n",
        "# Load the dataframes\n",
        "geo_df = pd.read_csv(GEO_URL)\n",
        "prod_df = pd.read_csv(PROD_URL)\n",
        "\n",
        "\n",
        "# Kolom yang digunakan\n",
        "geo_columns = {\n",
        "    \"Dataset\": \"Geolocation Dataset\",\n",
        "    \"ID\": \"geolocation_zip_code_prefix\",\n",
        "    \"Name\": \"geolocation_city\",\n",
        "    \"Quantity\": \"geolocation_lat\"\n",
        "}\n",
        "\n",
        "prod_columns = {\n",
        "    \"Dataset\": \"Products Dataset\",\n",
        "    \"ID\": \"product_id\",\n",
        "    \"Name\": \"product_category_name\",\n",
        "    \"Quantity\": \"product_length_cm\"\n",
        "}\n",
        "\n",
        "# Binary Search\n",
        "def benchmark_binary_search(df, column, target):\n",
        "    df_sorted = df.sort_values(by=column)\n",
        "    data = df_sorted[column].dropna().values\n",
        "    start = time.time()\n",
        "    low, high = 0, len(data) - 1\n",
        "    while low <= high:\n",
        "        mid = (low + high) // 2\n",
        "        if str(data[mid]) == str(target):\n",
        "            break\n",
        "        elif str(data[mid]) < str(target):\n",
        "            low = mid + 1\n",
        "        else:\n",
        "            high = mid - 1\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "# Benchmark sorting dan binary search\n",
        "def run_detailed_benchmark(df, dataset_info):\n",
        "    results = []\n",
        "    for op, col in [(\"Sort by\", \"ID\"), (\"Sort by\", \"Name\"), (\"Sort by\", \"Quantity\")]:\n",
        "        try:\n",
        "            start = time.time()\n",
        "            df.sort_values(by=dataset_info[col], inplace=False)\n",
        "            elapsed = round(time.time() - start, 6)\n",
        "        except:\n",
        "            elapsed = \"-\"\n",
        "        results.append([dataset_info[\"Dataset\"], dataset_info[col], f\"{op} {col}\", elapsed])\n",
        "\n",
        "    for col in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        try:\n",
        "            target = df[dataset_info[col]].dropna().iloc[len(df)//2]\n",
        "            elapsed = round(benchmark_binary_search(df, dataset_info[col], target), 6)\n",
        "        except:\n",
        "            elapsed = \"-\"\n",
        "        results.append([dataset_info[\"Dataset\"], dataset_info[col], f\"Binary Search {col}\", elapsed])\n",
        "\n",
        "    return results\n",
        "\n",
        "# Buat ringkasan akhir\n",
        "def summarize_benchmark(df, dataset_info, benchmark_results):\n",
        "    row_count = len(df)\n",
        "    summary = {\n",
        "        \"Dataset\": dataset_info[\"Dataset\"],\n",
        "        \"Rows\": row_count\n",
        "    }\n",
        "\n",
        "    for role in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        original_col = dataset_info[role]\n",
        "        summary[f\"{role} Column\"] = original_col\n",
        "        summary[f\"{role} Used As\"] = role\n",
        "\n",
        "        # Ambil waktu sort\n",
        "        sort_key = f\"Sort by {role}\"\n",
        "        sort_time = next((r[3] for r in benchmark_results if r[1] == original_col and r[2] == sort_key), 0)\n",
        "\n",
        "        # Ambil waktu search\n",
        "        search_key = f\"Binary Search {role}\"\n",
        "        search_time = next((r[3] for r in benchmark_results if r[1] == original_col and r[2] == search_key), 0)\n",
        "\n",
        "        # Simpan masing-masing waktu\n",
        "        summary[sort_key] = sort_time\n",
        "        summary[search_key] = search_time\n",
        "\n",
        "        # Total waktu (sort + search)\n",
        "        if isinstance(sort_time, (int, float)) and isinstance(search_time, (int, float)):\n",
        "            total_time = round(sort_time + search_time, 6)\n",
        "        else:\n",
        "            total_time = \"-\"\n",
        "        summary[f\"Total {role}\"] = total_time\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Jalankan benchmark\n",
        "geo_results = run_detailed_benchmark(geo_df, geo_columns)\n",
        "prod_results = run_detailed_benchmark(prod_df, prod_columns)\n",
        "\n",
        "# Buat summary tabel\n",
        "geo_summary = summarize_benchmark(geo_df, geo_columns, geo_results)\n",
        "prod_summary = summarize_benchmark(prod_df, prod_columns, prod_results)\n",
        "\n",
        "# Gabungkan dan tampilkan\n",
        "summary_df = pd.DataFrame([geo_summary, prod_summary])\n",
        "print(tabulate(summary_df, headers='keys', tablefmt='grid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jump Search - Heap Sort"
      ],
      "metadata": {
        "id": "l9vHI2zr5EcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "from tabulate import tabulate\n",
        "import heapq\n",
        "\n",
        "# Load dataset\n",
        "GEO_URL = '/content/drive/MyDrive/Kuliah/Analgo/Dataset/olist_geolocation_dataset.csv'\n",
        "PROD_URL = '/content/drive/MyDrive/Kuliah/Analgo/Dataset/olist_products_dataset.csv'\n",
        "\n",
        "# Kolom yang digunakan\n",
        "geo_columns = {\n",
        "    \"Dataset\": \"Geolocation Dataset\",\n",
        "    \"ID\": \"geolocation_zip_code_prefix\",\n",
        "    \"Name\": \"geolocation_city\",\n",
        "    \"Quantity\": \"geolocation_lat\"\n",
        "}\n",
        "\n",
        "prod_columns = {\n",
        "    \"Dataset\": \"Products Dataset\",\n",
        "    \"ID\": \"product_id\",\n",
        "    \"Name\": \"product_category_name\",\n",
        "    \"Quantity\": \"product_length_cm\"\n",
        "}\n",
        "\n",
        "# Heap Sort\n",
        "def benchmark_heap_sort(df, column):\n",
        "    data = df[column].dropna().tolist()\n",
        "    start = time.time()\n",
        "    heapq.heapify(data)\n",
        "    sorted_data = [heapq.heappop(data) for _ in range(len(data))]\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "# Jump Search\n",
        "def jump_search(arr, x):\n",
        "    n = len(arr)\n",
        "    step = int(math.sqrt(n))\n",
        "    prev = 0\n",
        "    while prev < n and arr[min(step, n)-1] < x:\n",
        "        prev = step\n",
        "        step += int(math.sqrt(n))\n",
        "        if prev >= n:\n",
        "            return -1\n",
        "    for i in range(prev, min(step, n)):\n",
        "        if arr[i] == x:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def benchmark_jump_search(df, column, target):\n",
        "    data = sorted(df[column].dropna().astype(str).values)\n",
        "    start = time.time()\n",
        "    jump_search(data, str(target))\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "# Benchmark per dataset\n",
        "def run_detailed_benchmark(df, dataset_info):\n",
        "    results = []\n",
        "    for col in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        try:\n",
        "            sort_time = benchmark_heap_sort(df, dataset_info[col])\n",
        "        except:\n",
        "            sort_time = \"-\"\n",
        "        results.append([dataset_info[\"Dataset\"], dataset_info[col], f\"Heap Sort {col}\", round(sort_time, 6)])\n",
        "\n",
        "    for col in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        try:\n",
        "            target = df[dataset_info[col]].dropna().iloc[len(df)//2]\n",
        "            search_time = benchmark_jump_search(df, dataset_info[col], target)\n",
        "        except:\n",
        "            search_time = \"-\"\n",
        "        results.append([dataset_info[\"Dataset\"], dataset_info[col], f\"Jump Search {col}\", round(search_time, 6)])\n",
        "\n",
        "    return results\n",
        "\n",
        "# Ringkasan benchmark\n",
        "def summarize_benchmark(df, dataset_info, benchmark_results):\n",
        "    row_count = len(df)\n",
        "    summary = {\n",
        "        \"Dataset\": dataset_info[\"Dataset\"],\n",
        "        \"Rows\": row_count\n",
        "    }\n",
        "\n",
        "    for role in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        original_col = dataset_info[role]\n",
        "        summary[f\"{role} Column\"] = original_col\n",
        "        summary[f\"{role} Used As\"] = role\n",
        "\n",
        "        # Ambil waktu heap sort\n",
        "        sort_key = f\"Heap Sort {role}\"\n",
        "        sort_time = next((r[3] for r in benchmark_results if r[1] == original_col and r[2] == sort_key), 0)\n",
        "\n",
        "        # Ambil waktu jump search\n",
        "        search_key = f\"Jump Search {role}\"\n",
        "        search_time = next((r[3] for r in benchmark_results if r[1] == original_col and r[2] == search_key), 0)\n",
        "\n",
        "        # Simpan masing-masing waktu\n",
        "        summary[sort_key] = sort_time\n",
        "        summary[search_key] = search_time\n",
        "\n",
        "        # Hitung total waktu (sort + search)\n",
        "        if isinstance(sort_time, (int, float)) and isinstance(search_time, (int, float)):\n",
        "            total_time = round(sort_time + search_time, 6)\n",
        "        else:\n",
        "            total_time = \"-\"\n",
        "        summary[f\"Total {role}\"] = total_time\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Jalankan benchmark\n",
        "geo_results = run_detailed_benchmark(geo_df, geo_columns)\n",
        "prod_results = run_detailed_benchmark(prod_df, prod_columns)\n",
        "\n",
        "# Tampilkan tabel hasil\n",
        "geo_summary = summarize_benchmark(geo_df, geo_columns, geo_results)\n",
        "prod_summary = summarize_benchmark(prod_df, prod_columns, prod_results)\n",
        "\n",
        "summary_df = pd.DataFrame([geo_summary, prod_summary])\n",
        "print(tabulate(summary_df, headers='keys', tablefmt='grid'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe_Gcxa-5MBJ",
        "outputId": "f1b46098-5528-4e7d-847b-a98df9a47dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+---------+-----------------------------+--------------+----------------+------------------+------------+-----------------------+----------------+------------------+--------------------+--------------+-------------------+--------------------+----------------------+------------------------+------------------+\n",
            "|    | Dataset             |    Rows | ID Column                   | ID Used As   |   Heap Sort ID |   Jump Search ID |   Total ID | Name Column           | Name Used As   |   Heap Sort Name |   Jump Search Name |   Total Name | Quantity Column   | Quantity Used As   |   Heap Sort Quantity |   Jump Search Quantity |   Total Quantity |\n",
            "+====+=====================+=========+=============================+==============+================+==================+============+=======================+================+==================+====================+==============+===================+====================+======================+========================+==================+\n",
            "|  0 | Geolocation Dataset | 1000163 | geolocation_zip_code_prefix | ID           |       1.11679  |         0.000348 |    1.11713 | geolocation_city      | Name           |         1.22122  |           0.000325 |     1.22154  | geolocation_lat   | Quantity           |             1.48156  |                0.00065 |         1.48221  |\n",
            "+----+---------------------+---------+-----------------------------+--------------+----------------+------------------+------------+-----------------------+----------------+------------------+--------------------+--------------+-------------------+--------------------+----------------------+------------------------+------------------+\n",
            "|  1 | Products Dataset    |   32951 | product_id                  | ID           |       0.022002 |         5.8e-05  |    0.02206 | product_category_name | Name           |         0.015298 |           4.1e-05  |     0.015339 | product_length_cm | Quantity           |             0.013106 |                7.2e-05 |         0.013178 |\n",
            "+----+---------------------+---------+-----------------------------+--------------+----------------+------------------+------------+-----------------------+----------------+------------------+--------------------+--------------+-------------------+--------------------+----------------------+------------------------+------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jump Sort - Merge Sort"
      ],
      "metadata": {
        "id": "-Yxmf-0L7s26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load dataset\n",
        "GEO_URL = '/content/drive/MyDrive/Kuliah/Analgo/Dataset/olist_geolocation_dataset.csv'\n",
        "PROD_URL = '/content/drive/MyDrive/Kuliah/Analgo/Dataset/olist_products_dataset.csv'\n",
        "\n",
        "# Kolom yang digunakan\n",
        "geo_columns = {\n",
        "    \"Dataset\": \"Geolocation Dataset\",\n",
        "    \"ID\": \"geolocation_zip_code_prefix\",\n",
        "    \"Name\": \"geolocation_city\",\n",
        "    \"Quantity\": \"geolocation_lat\"\n",
        "}\n",
        "\n",
        "prod_columns = {\n",
        "    \"Dataset\": \"Products Dataset\",\n",
        "    \"ID\": \"product_id\",\n",
        "    \"Name\": \"product_category_name\",\n",
        "    \"Quantity\": \"product_length_cm\"\n",
        "}\n",
        "\n",
        "# Merge Sort\n",
        "def merge_sort(arr):\n",
        "    if len(arr) <= 1:\n",
        "        return arr\n",
        "    mid = len(arr) // 2\n",
        "    left = merge_sort(arr[:mid])\n",
        "    right = merge_sort(arr[mid:])\n",
        "    return merge(left, right)\n",
        "\n",
        "def merge(left, right):\n",
        "    merged = []\n",
        "    i = j = 0\n",
        "    while i < len(left) and j < len(right):\n",
        "        if str(left[i]) <= str(right[j]):\n",
        "            merged.append(left[i])\n",
        "            i += 1\n",
        "        else:\n",
        "            merged.append(right[j])\n",
        "            j += 1\n",
        "    merged += left[i:]\n",
        "    merged += right[j:]\n",
        "    return merged\n",
        "\n",
        "def benchmark_merge_sort(df, column):\n",
        "    data = df[column].dropna().tolist()\n",
        "    start = time.time()\n",
        "    merge_sort(data)\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "# Jump Search\n",
        "def jump_search(arr, x):\n",
        "    n = len(arr)\n",
        "    step = int(math.sqrt(n))\n",
        "    prev = 0\n",
        "    while prev < n and arr[min(step, n)-1] < x:\n",
        "        prev = step\n",
        "        step += int(math.sqrt(n))\n",
        "        if prev >= n:\n",
        "            return -1\n",
        "    for i in range(prev, min(step, n)):\n",
        "        if arr[i] == x:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def benchmark_jump_search(df, column, target):\n",
        "    data = sorted(df[column].dropna().astype(str).values)\n",
        "    start = time.time()\n",
        "    jump_search(data, str(target))\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "# Benchmark runner\n",
        "def run_detailed_benchmark(df, dataset_info):\n",
        "    results = []\n",
        "    for col in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        try:\n",
        "            sort_time = benchmark_merge_sort(df, dataset_info[col])\n",
        "        except:\n",
        "            sort_time = \"-\"\n",
        "        results.append([\n",
        "            dataset_info[\"Dataset\"],\n",
        "            dataset_info[col],\n",
        "            f\"Merge Sort {col}\",\n",
        "            f\"{sort_time:.6f}\" if isinstance(sort_time, float) else \"-\"\n",
        "        ])\n",
        "\n",
        "    for col in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        try:\n",
        "            target = df[dataset_info[col]].dropna().iloc[len(df) // 2]\n",
        "            search_time = benchmark_jump_search(df, dataset_info[col], target)\n",
        "        except:\n",
        "            search_time = \"-\"\n",
        "        results.append([\n",
        "            dataset_info[\"Dataset\"],\n",
        "            dataset_info[col],\n",
        "            f\"Jump Search {col}\",\n",
        "            f\"{search_time:.6f}\" if isinstance(search_time, float) else \"-\"\n",
        "        ])\n",
        "    return results\n",
        "\n",
        "# Ringkasan hasil benchmark\n",
        "def summarize_benchmark(df, dataset_info, benchmark_results):\n",
        "    row_count = len(df)\n",
        "    summary = {\n",
        "        \"Dataset\": dataset_info[\"Dataset\"],\n",
        "        \"Rows\": row_count\n",
        "    }\n",
        "\n",
        "    for role in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        original_col = dataset_info[role]\n",
        "        summary[f\"{role} Column\"] = original_col\n",
        "        summary[f\"{role} Used As\"] = role\n",
        "\n",
        "        sort_key = f\"Merge Sort {role}\"\n",
        "        search_key = f\"Jump Search {role}\"\n",
        "\n",
        "        sort_time = next((r[3] for r in benchmark_results if r[1] == original_col and r[2] == sort_key), \"0\")\n",
        "        search_time = next((r[3] for r in benchmark_results if r[1] == original_col and r[2] == search_key), \"0\")\n",
        "\n",
        "        try:\n",
        "            sort_time = f\"{float(sort_time):.6f}\"\n",
        "        except:\n",
        "            sort_time = \"-\"\n",
        "        try:\n",
        "            search_time = f\"{float(search_time):.6f}\"\n",
        "        except:\n",
        "            search_time = \"-\"\n",
        "\n",
        "        summary[sort_key] = sort_time\n",
        "        summary[search_key] = search_time\n",
        "\n",
        "        try:\n",
        "            total_time = float(sort_time) + float(search_time)\n",
        "            summary[f\"Total {role}\"] = f\"{total_time:.6f}\"\n",
        "        except:\n",
        "            summary[f\"Total {role}\"] = \"-\"\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Jalankan benchmark\n",
        "geo_results = run_detailed_benchmark(geo_df, geo_columns)\n",
        "prod_results = run_detailed_benchmark(prod_df, prod_columns)\n",
        "\n",
        "geo_summary = summarize_benchmark(geo_df, geo_columns, geo_results)\n",
        "prod_summary = summarize_benchmark(prod_df, prod_columns, prod_results)\n",
        "\n",
        "summary_df = pd.DataFrame([geo_summary, prod_summary])\n",
        "\n",
        "# Pastikan semua waktu dalam format string desimal, bukan notasi ilmiah\n",
        "for col in summary_df.columns:\n",
        "    summary_df[col] = summary_df[col].apply(lambda x: f\"{float(x):.6f}\" if isinstance(x, float) else x)\n",
        "\n",
        "# Tampilkan tabel\n",
        "print(tabulate(summary_df, headers='keys', tablefmt='grid'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YJgprPk77TE",
        "outputId": "f6c74149-0d45-4322-eb8d-272a2a68fae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+---------+-----------------------------+--------------+-----------------+------------------+------------+-----------------------+----------------+-------------------+--------------------+--------------+-------------------+--------------------+-----------------------+------------------------+------------------+\n",
            "|    | Dataset             |    Rows | ID Column                   | ID Used As   |   Merge Sort ID |   Jump Search ID |   Total ID | Name Column           | Name Used As   |   Merge Sort Name |   Jump Search Name |   Total Name | Quantity Column   | Quantity Used As   |   Merge Sort Quantity |   Jump Search Quantity |   Total Quantity |\n",
            "+====+=====================+=========+=============================+==============+=================+==================+============+=======================+================+===================+====================+==============+===================+====================+=======================+========================+==================+\n",
            "|  0 | Geolocation Dataset | 1000163 | geolocation_zip_code_prefix | ID           |        5.72723  |         0.000375 |   5.7276   | geolocation_city      | Name           |          6.81654  |           0.000246 |     6.81679  | geolocation_lat   | Quantity           |             33.4498   |               0.000407 |         33.4502  |\n",
            "+----+---------------------+---------+-----------------------------+--------------+-----------------+------------------+------------+-----------------------+----------------+-------------------+--------------------+--------------+-------------------+--------------------+-----------------------+------------------------+------------------+\n",
            "|  1 | Products Dataset    |   32951 | product_id                  | ID           |        0.183936 |         5.5e-05  |   0.183991 | product_category_name | Name           |          0.114473 |           4.5e-05  |     0.114518 | product_length_cm | Quantity           |              0.245015 |               6.5e-05  |          0.24508 |\n",
            "+----+---------------------+---------+-----------------------------+--------------+-----------------+------------------+------------+-----------------------+----------------+-------------------+--------------------+--------------+-------------------+--------------------+-----------------------+------------------------+------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hash Search - Selection Sort"
      ],
      "metadata": {
        "id": "GT2_kPtq_elg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load datasets\n",
        "GEO_URL = '/content/drive/MyDrive/Kuliah/Analgo/Dataset/olist_geolocation_dataset.csv'\n",
        "PROD_URL = '/content/drive/MyDrive/Kuliah/Analgo/Dataset/olist_products_dataset.csv'\n",
        "\n",
        "# Load the dataframes\n",
        "geo_df = pd.read_csv(GEO_URL)\n",
        "prod_df = pd.read_csv(PROD_URL)\n",
        "\n",
        "# Auto sample function with 20,000 limit\n",
        "def auto_sample(df, max_rows=20000):\n",
        "    return df.sample(n=max_rows, random_state=42) if len(df) > max_rows else df.copy()\n",
        "\n",
        "# Define columns used\n",
        "geo_columns = {\n",
        "    \"Dataset\": \"Geolocation Dataset\",\n",
        "    \"ID\": \"geolocation_zip_code_prefix\",\n",
        "    \"Name\": \"geolocation_city\",\n",
        "    \"Quantity\": \"geolocation_lat\"\n",
        "}\n",
        "\n",
        "prod_columns = {\n",
        "    \"Dataset\": \"Products Dataset\",\n",
        "    \"ID\": \"product_id\",\n",
        "    \"Name\": \"product_category_name\",\n",
        "    \"Quantity\": \"product_length_cm\"\n",
        "}\n",
        "\n",
        "# Selection Sort\n",
        "def selection_sort(arr):\n",
        "    arr = arr.copy()\n",
        "    for i in range(len(arr)):\n",
        "        min_idx = i\n",
        "        for j in range(i+1, len(arr)):\n",
        "            if str(arr[j]) < str(arr[min_idx]):\n",
        "                min_idx = j\n",
        "        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n",
        "    return arr\n",
        "\n",
        "def benchmark_selection_sort(df, column):\n",
        "    data = df[column].dropna().tolist()\n",
        "    start = time.time()\n",
        "    selection_sort(data)\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "# Hash Search\n",
        "def hash_search(data_dict, target):\n",
        "    return data_dict.get(target, None)\n",
        "\n",
        "def benchmark_hash_search(df, column, target):\n",
        "    data = df[column].dropna().astype(str).tolist()\n",
        "    hash_map = {val: i for i, val in enumerate(data)}\n",
        "    start = time.time()\n",
        "    hash_search(hash_map, str(target))\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "# Benchmark runner\n",
        "def run_detailed_benchmark(df, dataset_info):\n",
        "    df = auto_sample(df)  # apply sampling\n",
        "    results = []\n",
        "    for col in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        try:\n",
        "            sort_time = benchmark_selection_sort(df, dataset_info[col])\n",
        "        except Exception as e: # Catch general exception to prevent crash, useful for debugging\n",
        "            print(f\"Error during Selection Sort for column {dataset_info[col]}: {e}\")\n",
        "            sort_time = \"-\"\n",
        "        results.append([\n",
        "            dataset_info[\"Dataset\"],\n",
        "            dataset_info[col],\n",
        "            f\"Selection Sort {col}\",\n",
        "            f\"{sort_time:.6f}\" if isinstance(sort_time, float) else \"-\"\n",
        "        ])\n",
        "\n",
        "    for col in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        try:\n",
        "            # Ensure target is sampled appropriately from the potentially sampled df\n",
        "            target = df[dataset_info[col]].dropna().iloc[len(df) // 2]\n",
        "            search_time = benchmark_hash_search(df, dataset_info[col], target)\n",
        "        except Exception as e: # Catch general exception\n",
        "            print(f\"Error during Hash Search for column {dataset_info[col]}: {e}\")\n",
        "            search_time = \"-\"\n",
        "        results.append([\n",
        "            dataset_info[\"Dataset\"],\n",
        "            dataset_info[col],\n",
        "            f\"Hash Search {col}\",\n",
        "            f\"{search_time:.6f}\" if isinstance(search_time, float) else \"-\"\n",
        "        ])\n",
        "    return results\n",
        "\n",
        "# Summarize benchmark\n",
        "def summarize_benchmark(df, dataset_info, benchmark_results):\n",
        "    row_count = len(df)\n",
        "    summary = {\n",
        "        \"Dataset\": dataset_info[\"Dataset\"],\n",
        "        \"Rows\": row_count\n",
        "    }\n",
        "\n",
        "    for role in [\"ID\", \"Name\", \"Quantity\"]:\n",
        "        original_col = dataset_info[role]\n",
        "        summary[f\"{role} Column\"] = original_col\n",
        "        summary[f\"{role} Used As\"] = role\n",
        "\n",
        "        sort_key = f\"Selection Sort {role}\"\n",
        "        search_key = f\"Hash Search {role}\"\n",
        "\n",
        "        sort_time = next((r[3] for r in benchmark_results if r[1] == original_col and r[2] == sort_key), \"0\")\n",
        "        search_time = next((r[3] for r in benchmark_results if r[1] == original_col and r[2] == search_key), \"0\")\n",
        "\n",
        "        try:\n",
        "            sort_time = f\"{float(sort_time):.6f}\"\n",
        "        except:\n",
        "            sort_time = \"-\"\n",
        "        try:\n",
        "            search_time = f\"{float(search_time):.6f}\"\n",
        "        except:\n",
        "            search_time = \"-\"\n",
        "\n",
        "        summary[sort_key] = sort_time\n",
        "        summary[search_key] = search_time\n",
        "\n",
        "        try:\n",
        "            # Convert strings back to floats for calculation, handle \"-\"\n",
        "            sort_time_float = float(sort_time) if sort_time != \"-\" else 0\n",
        "            search_time_float = float(search_time) if search_time != \"-\" else 0\n",
        "            total_time = sort_time_float + search_time_float\n",
        "\n",
        "            # Check if either was \"-\" and set total to \"-\" if so\n",
        "            if sort_time == \"-\" or search_time == \"-\":\n",
        "                 summary[f\"Total {role}\"] = \"-\"\n",
        "            else:\n",
        "                 summary[f\"Total {role}\"] = f\"{total_time:.6f}\"\n",
        "        except:\n",
        "            summary[f\"Total {role}\"] = \"-\" # Fallback in case of unexpected issue\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Run benchmarks\n",
        "# Pass the potentially sampled dataframes to the summary functions as well\n",
        "geo_sampled_df = auto_sample(geo_df)\n",
        "prod_sampled_df = auto_sample(prod_df)\n",
        "\n",
        "geo_results = run_detailed_benchmark(geo_df, geo_columns)\n",
        "prod_results = run_detailed_benchmark(prod_df, prod_columns)\n",
        "\n",
        "# Use the sampled dataframes for summarizing to match row count\n",
        "geo_summary = summarize_benchmark(geo_sampled_df, geo_columns, geo_results)\n",
        "prod_summary = summarize_benchmark(prod_sampled_df, prod_columns, prod_results)\n",
        "\n",
        "\n",
        "summary_df = pd.DataFrame([geo_summary, prod_summary])\n",
        "\n",
        "# Show table using tabulate\n",
        "# No longer need to format here as it's done in summarize_benchmark\n",
        "print(tabulate(summary_df, headers='keys', tablefmt='grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuevRqffMXgB",
        "outputId": "0c7f6464-987c-4381-e0bc-a62cd065b4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+--------+-----------------------------+--------------+---------------------+------------------+------------+-----------------------+----------------+-----------------------+--------------------+--------------+-------------------+--------------------+---------------------------+------------------------+------------------+\n",
            "|    | Dataset             |   Rows | ID Column                   | ID Used As   |   Selection Sort ID |   Hash Search ID |   Total ID | Name Column           | Name Used As   |   Selection Sort Name |   Hash Search Name |   Total Name | Quantity Column   | Quantity Used As   |   Selection Sort Quantity |   Hash Search Quantity |   Total Quantity |\n",
            "+====+=====================+========+=============================+==============+=====================+==================+============+=======================+================+=======================+====================+==============+===================+====================+===========================+========================+==================+\n",
            "|  0 | Geolocation Dataset |  20000 | geolocation_zip_code_prefix | ID           |             50.1195 |          1.1e-05 |    50.1195 | geolocation_city      | Name           |               23.7345 |              4e-06 |      23.7345 | geolocation_lat   | Quantity           |                  330.993  |                1.2e-05 |         330.993  |\n",
            "+----+---------------------+--------+-----------------------------+--------------+---------------------+------------------+------------+-----------------------+----------------+-----------------------+--------------------+--------------+-------------------+--------------------+---------------------------+------------------------+------------------+\n",
            "|  1 | Products Dataset    |  20000 | product_id                  | ID           |             23.8476 |          7e-06   |    23.8476 | product_category_name | Name           |               22.8097 |              5e-06 |      22.8097 | product_length_cm | Quantity           |                   80.7811 |                9e-06   |          80.7811 |\n",
            "+----+---------------------+--------+-----------------------------+--------------+---------------------+------------------+------------+-----------------------+----------------+-----------------------+--------------------+--------------+-------------------+--------------------+---------------------------+------------------------+------------------+\n"
          ]
        }
      ]
    }
  ]
}